{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "from pprint import pprint\n",
    "\n",
    "### Set working directory to parent directory to import fields processing functions\n",
    "os.chdir(os.path.pardir)\n",
    "\n",
    "cwd = os.getcwd()\n",
    "print(cwd)\n",
    "\n",
    "py_files = glob('*.py')\n",
    "print(py_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Config parameters are stroed in the global_config.py, imported at the top of the fields_functions.py\n",
    "### Import fields functions\n",
    "from fields_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prep_file_dir': 'C:/Users/jesse/Documents/grad school/masters research/code/fields_library/data/rasters/from_MSI/', 'prep_tile_id': 'TPT', 'prep_base_chunk': 'auto', 'prep_time_chunk': 'auto', 'prep_remove_overlap': False, 'prep_manual_subset': True, 'prep_x_start': 7500, 'prep_y_start': 7500, 'prep_step': 500, 'prep_cloud_coverage_thresh': 50, 'prep_load_cloud_mask': True, 'prep_apply_cloud_mask': True, 'prep_cloud_mask_thresh': 70, 'prep_clip_outliers': True, 'prep_clip_percentile': 1, 'prep_normalize_bands': True, 'preproc_out_dir': 'preproc_out_dir/', 'preproc_outfile_prefix': 'fields_preproc_demo_', 'preproc_sample_pct': 0.05, 'preproc_n_clusters': 15, 'preproc_cluster_tile': True, 'kmeans_n_clusters': 15, 'kmeans_model_out_dir': 'kmeans_model_dir/', 'kmeans_8var_clusters': True, 'kmeans_std_thresh': 0.2, 'kmeans_min_thresh': 0, 'kmeans_max_thresh': 0.3, 'kmeans_range_thresh': 0.7, 'kmeans_ndwi_thresh': 0.2, 'kmeans_mask_out_dir': 'mask_out_dir/', 'kmeans_from_full_tile_mask': False, 'seg_rgb_date_str': '20190819', 'seg_rgb_gaussian_filt': False, 'seg_rgb_gaussian_sigma': 1, 'seg_rgb_percentile': 1, 'seg_use_nir': True, 'seg_fz_scale': 200, 'seg_fz_sigma': 0.5, 'seg_fz_min_size': 400, 'shp_out_dir': 'shp_dir/', 'shp_file_out_str': '_code_demo_clustering'}\n"
     ]
    }
   ],
   "source": [
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import memory_profiler\n",
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 208.25 MiB, increment: 0.04 MiB\n",
      "memory test\n"
     ]
    }
   ],
   "source": [
    "%memit\n",
    "\n",
    "print('memory test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "-------------------------------------\n",
    "Mask Processing functions\n",
    "-------------------------------------\n",
    "\"\"\"\n",
    "\n",
    "### QUESTION : the data in each band gets normalized multiple times (red & nir in NDVI, green & nir in NDWI, and all bands in the edge map)\n",
    "###             should this step happen once up front instead?\n",
    "\n",
    "### NDVI\n",
    "def ndvi_xr(input_ds):\n",
    "    '''\n",
    "    computes the Normalized Difference Vegetation Index\n",
    "    '''\n",
    "    np.seterr(divide='ignore', invalid='ignore')\n",
    "    return ((input_ds.nir - input_ds.red)/(input_ds.nir + input_ds.red))\n",
    "\n",
    "### UPDATE : the clip and normalize functions have been moved to the data prep step, so it shouldn't be necessary here\n",
    "def ndvi_xr_norm(input_ds):\n",
    "    np.seterr(divide='ignore', invalid='ignore')\n",
    "    nir = normalize_ufunc(input_ds.nir)\n",
    "    red = normalize_ufunc(input_ds.red)\n",
    "    return ((nir - red)/(nir + red))\n",
    "\n",
    "### EVI - // Enhanced Vegetation Index  (abbrv. EVI)\n",
    "#// General formula: 2.5 * (NIR - RED) / ((NIR + 6*RED - 7.5*BLUE) + 1)\n",
    "#// URL https://www.indexdatabase.de/db/si-single.php?sensor_id=96&rsindex_id=16\n",
    "def evi_xr(input_ds):\n",
    "    '''\n",
    "    computes the Enhanced Vegetation Index\n",
    "    '''\n",
    "    np.seterr(divide='ignore', invalid='ignore')    \n",
    "    input_ds = clip_nan_ufunc(input_ds, 1)\n",
    "    input_ds = normalize_ufunc(input_ds)\n",
    "    EVI = 2.5 * (input_ds.nir - input_ds.red) / ((input_ds.nir + 6.0 * input_ds.red - 7.5 * input_ds.blue) + 1.0)\n",
    "    return EVI\n",
    "\n",
    "### GCVI - green chlorophyll vegetation index\n",
    "### https://digitalcommons.unl.edu/cgi/viewcontent.cgi?article=1278&context=natrespapers\n",
    "### (NIR - green) / 1\n",
    "### This isn't used\n",
    "def gcvi_xr(input_ds):\n",
    "    np.seterr(divide='ignore', invalid='ignore')\n",
    "    return ((input_ds.nir - input_ds.green) - 1)\n",
    "\n",
    "### NDWI\n",
    "def ndwi_xr(input_ds):\n",
    "    \"\"\"\n",
    "    computes the Normalized Difference Water Index\n",
    "    \"\"\"\n",
    "    np.seterr(divide='ignore', invalid='ignore')\n",
    "    return ((input_ds.green - input_ds.nir)/(input_ds.green + input_ds.nir))\n",
    "\n",
    "### UPDATE : the clip and normalize functions have been moved to the data prep step, so it shouldn't be necessary here\n",
    "def ndwi_xr_norm(input_ds):\n",
    "    \"\"\"\n",
    "    computes the Normalized Difference Water Index\n",
    "    \"\"\"\n",
    "    np.seterr(divide='ignore', invalid='ignore')\n",
    "    green = normalize_ufunc(input_ds.green)\n",
    "    nir = normalize_ufunc(input_ds.nir)\n",
    "    return ((green - nir)/(green + nir))\n",
    "\n",
    "### Find threshold value for NDWI - 0.5 to start\n",
    "    ### UPDATE : switched from ndwi_xr_norm() to ndwi_xr() because the normalization takes place in the data prep now\n",
    "def ndwi_mask_func(input_ds, ndwi_thresh = 0.5):\n",
    "    ### Take the mean NDWI value across the full time stack, normalized  \n",
    "    return xr.where(ndwi_xr_norm(input_ds).mean(dim='time', skipna=True) > ndwi_thresh, 1, 0).compute()\n",
    "#    return xr.where(ndwi_xr(input_ds).mean(dim='time', skipna=True) > ndwi_thresh, 1, 0)\n",
    "\n",
    "### EDGES\n",
    "# Sobel edges function from skimage\n",
    "def sobel_edges(input_array):\n",
    "    return sobel(input_array)\n",
    "\n",
    "\n",
    "### Prior version of the edges algorithm implementation did not provide accurate outputs, emphasizing noise in the time stack\n",
    "def compute_edges(ds_time_stack, edge_algo = sobel_edges, chunk_size='auto', percentile=0.1):\n",
    "    for i in np.arange(0,len(ds_time_stack.coords['time']),1):\n",
    "        edges = xr.ufuncs.fabs(ds_time_stack.isel(time=i).fillna(0).map(edge_algo)).chunk({'x':chunk_size,'y':chunk_size})\n",
    "        if i == 0:\n",
    "            edges_sum = (edges['red']+edges['green']+edges['blue']+edges['nir'])\n",
    "        if i > 0:\n",
    "            edges_sum = edges_sum + (edges['red']+edges['green']+edges['blue']+edges['nir'])\n",
    "        \n",
    "#    edges_sum = normalize_ufunc(clip_nan_ufunc(edges_sum/(4*len(ds_time_stack.coords['time'])), percentile))\n",
    "    edges_sum = edges_sum/(4*len(ds_time_stack.coords['time']))\n",
    "    edges_sum = clip_nan_ufunc(edges_sum, percentile)\n",
    "    edges_sum = normalize_ufunc(edges_sum)\n",
    "\n",
    "    return edges_sum\n",
    "\n",
    "\"\"\"\n",
    "These functions are from the original thresholding approach\n",
    "The cumulative edge algorithm here is slower than the one above (I think)\n",
    "The edge finding approach has a lot of room for improvement.\n",
    "\"\"\"\n",
    "# Passes each band in each timestep of a xr dataset through the sobel edge filter in parallel, lazily evaluated\n",
    "# Concatenates the sum of each timestep's edges into a xr dataarray in teh same shape as the input\n",
    "### This could be broken up/made more modular for cases where there is only a single timestep, for instance\n",
    "def edges_to_xr_data_array(input_ds, ndvi = False):\n",
    "    # Define some lists to get populated in the loop\n",
    "    edges_list = []\n",
    "    edges_time_list = []\n",
    "    \n",
    "    bands = ['red','green','blue','nir']\n",
    "    \n",
    "    # List of bands to loop over\n",
    "    if ndvi:\n",
    "        bands = ['red','green','blue','nir','ndvi']\n",
    "    \n",
    "    # Loop through each time step to populate a list of summed edges for each band in each time step\n",
    "    for i in np.arange(0,len(input_ds.coords['time']),1):\n",
    "#        if input_ds.isnull:\n",
    "#            continue\n",
    "        \n",
    "        # Loop through each band to compute edges and append resulting dataarray to a list\n",
    "        for band in bands:\n",
    "            # define coords for \n",
    "            coords = [input_ds[band].coords['y'],input_ds[band].coords['x']]\n",
    "            \n",
    "            # Run the sobel edges algorithm on overlapping chunks to cover seams in chunks because it is a focal operation\n",
    "            # The input band (ds[band]) gets normalized so values range from 0 to 1 and then clipped to remove \n",
    "            # outliers at the highest and lowest 2% of values\n",
    "            # prep edges input by normalizing the band\n",
    "            edges_input_norm = normalize_ufunc(clip_nan_ufunc(input_ds[band].isel(time=i),2))\n",
    "            # lazily pass array through the sobel_edges() function via the dask.map_overlap() function\n",
    "            band_egdes_output = edges_input_norm.data.map_overlap(sobel_edges, depth=1)\n",
    "            \n",
    "            # Convert the edges array to a dataarray using the x and y coordinates of the input array\n",
    "            band_egdes_output_da = xr.DataArray(band_egdes_output,\n",
    "                                                dims=('y','x'),\n",
    "                                                coords=coords)\n",
    "            \n",
    "            # Add the time step dimension to the edges array that matches the time coordinate for the time step being processed\n",
    "            band_egdes_output_da['time'] = input_ds['time'][i]\n",
    "            \n",
    "            # append the edges to a list of DataArrays, one for each band for each time step\n",
    "            edges_list.append(band_egdes_output_da)\n",
    "        \n",
    "        edges_list_da = xr.concat(edges_list, dim='time')\n",
    "        # For each time step, sum the edges for each band\n",
    "#        edges_sum = edges_list_da.sum(dim='time', skipna=True)\n",
    "        edges_sum = edges_list_da.mean(dim='time', skipna=True)\n",
    "        # Give this new dataarray a logical name\n",
    "        edges_sum.name = 'edges_sum'\n",
    "        # Append the summed edge map of each time step to a list, to be merged along the 'time' dimension\n",
    "        edges_time_list.append(edges_sum)\n",
    "    # concat the summed edges for each time step into an array that matches the shape (x, y, time) of the time_stack_ds\n",
    "    edges_da = xr.concat(edges_time_list, dim='time')\n",
    "\n",
    "    return edges_da\n",
    "\n",
    "\n",
    "### build mask\n",
    "    \n",
    "def mask_ndvi_max_and_range(monthly_avg_ndvi_ds, max_thresh = 0.3, range_thresh = 0.3):\n",
    "    ### Maximum monthly average NDVI\n",
    "    ndvi_monthly_max = monthly_avg_ndvi_ds.max(dim='time', skipna = True)\n",
    "    ### NDVI range from max monthly NDVI to mean NDVI for May\n",
    "    ndvi_range_max_t0 = ndvi_monthly_max - monthly_avg_ndvi_ds.isel(time=0)\n",
    "    ndvi_range_mask = xr.where(ndvi_range_max_t0 < range_thresh, 1, 0)\n",
    "    ### Max monthly average ndvi\n",
    "    ndvi_max_mask = xr.where(ndvi_monthly_max < max_thresh, 1, 0)\n",
    "    ### Combine masks\n",
    "    return xr.ufuncs.logical_or(ndvi_range_mask, ndvi_max_mask)\n",
    "\n",
    "# This function is in the mask.py file\n",
    "def create_combined_ndvi_edges(ndvi, edges, ndvi_weight=1, edges_weight=1):\n",
    "    return (ndvi_weight*ndvi) + (edges_weight*edges)\n",
    "\n",
    "def create_mask_bool_array(ndvi_edge_combo, binary_threshold):\n",
    "    return xr.where(normalize_ufunc(ndvi_edge_combo) < binary_threshold, 1, 0)\n",
    "#    return xr.where(normalize_ufunc(ndvi_edge_combo) < binary_threshold, False, True)\n",
    "\n",
    "def build_mask(ndvi_range, cumulative_edges, ndvi_weight, edges_weight):\n",
    "    # inputs get normalized so that each range from 0 to 1\n",
    "    combined_mask_inputs = create_combined_ndvi_edges(ndvi = 1 - normalize_ufunc(ndvi_range), \n",
    "                                      edges = normalize_ufunc(cumulative_edges), \n",
    "                                      ndvi_weight = ndvi_weight, \n",
    "                                      edges_weight = edges_weight)\n",
    "\n",
    "    # Compute mask\n",
    "    return combined_mask_inputs\n",
    "\n",
    "# This will get passed to dask arrays within the dataset through dask.map_overlap() for parallelization\n",
    "def fill_holes(input_array):\n",
    "    return ndi.binary_fill_holes(input_array)\n",
    "\n",
    "# This will get passed to dask arrays within the dataset through dask.map_overlap() for parallelization\n",
    "### Original preliminary run used a size = (2,2)\n",
    "def min_filter(input_array, size = (2, 2)):\n",
    "    return ndi.minimum_filter(input_array, size=size)\n",
    "\n",
    "\n",
    "def mask_processing(input_ds, ndwi_thresh = 0.5, ndvi_max_thresh = 0.3, ndvi_range_thresh = 0.3, edges_thresh = 0.3):\n",
    "    ### NDWI mask: mean NDWI across all time steps, masked for values greater than ndwi_thresh\n",
    "    ndwi_mask = ndwi_mask_func(input_ds, ndwi_thresh = ndwi_thresh)\n",
    "    \n",
    "    ### NDVI Masking\n",
    "    # Compute monthly mean composites\n",
    "    monthly_avg = input_ds.resample(time='1MS').mean(skipna = True)\n",
    "    # Compute NDVI on the monthly composite iamges\n",
    "    monthly_avg_ndvi = ndvi_xr_norm(monthly_avg)\n",
    "    # Compute NDVI mask: Max monthly mean NDVI > max_thresh, and Max Monthly NDVI - May monthly mean NDVI \n",
    "    combined_ndvi_mask = mask_ndvi_max_and_range(monthly_avg_ndvi, \n",
    "                                                 max_thresh = ndvi_max_thresh, \n",
    "                                                 range_thresh = ndvi_range_thresh)\n",
    "    \n",
    "    ### Edges\n",
    "    # Mean edges for full data stack\n",
    "    edges_mean = normalize_ufunc(clip_nan_ufunc(edges_to_xr_data_array(input_ds).mean(dim='time', skipna = True), 2))\n",
    "    # Mask edges\n",
    "    edges_mask = xr.where(edges_mean > edges_thresh, 1, 0)\n",
    "    \n",
    "    \n",
    "    ### Combined mask\n",
    "    combined_mask = xr.ufuncs.logical_or(xr.ufuncs.logical_or(ndwi_mask, combined_ndvi_mask), edges_mask)\n",
    "    # Invert mask\n",
    "    mask_invert = xr.where(combined_mask == 1, 0, 1)\n",
    "    \n",
    "    ### Fill holes in the mask so that there aren't stray pixels\n",
    "    mask_holes_filled = mask_invert.data.map_overlap(fill_holes, depth=1)\n",
    "    \n",
    "    ### Set minimum filter to enforce minimum height/width of background. \n",
    "    ### This eliminates small isolated areas and expands road areas.\n",
    "    mask_minimum_filt = mask_holes_filled.map_overlap(min_filter, depth = 1)\n",
    "    \n",
    "    ### Set mask to ds_time_stack array\n",
    "    input_ds['mask'] = xr.DataArray(mask_minimum_filt, \n",
    "                                     dims=('y','x'),\n",
    "                                     coords = [input_ds['red'].coords['y'],\n",
    "                                               input_ds['red'].coords['x']])\n",
    "    \n",
    "    return input_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloud Coverage Threshold: 50\n",
      "FAILED cloud coverage: 2019 05 03 with  99.009547 pct | nodata pixel pct: 0.126635\n",
      "FAILED cloud coverage: 2019 05 06 with  72.591515 pct | nodata pixel pct: 5.957223\n",
      "passed cloud coverage: 2019 05 13 with 34.97001 pct | nodata pixel pct: 0.134791\n",
      "FAILED cloud coverage: 2019 05 16 with  72.077132 pct | nodata pixel pct: 5.796779\n",
      "FAILED cloud coverage: 2019 05 23 with  96.121381 pct | nodata pixel pct: 0.149615\n",
      "FAILED cloud coverage: 2019 05 26 with  73.563154 pct | nodata pixel pct: 5.719112\n",
      "passed cloud coverage: 2019 06 02 with 21.420233 pct | nodata pixel pct: 0.193563\n",
      "passed cloud coverage: 2019 06 05 with 3.328879 pct | nodata pixel pct: 5.51809\n",
      "passed cloud coverage: 2019 06 12 with 3.1731840000000004 pct | nodata pixel pct: 0.164777\n",
      "FAILED cloud coverage: 2019 06 15 with  91.997779 pct | nodata pixel pct: 5.732784\n",
      "FAILED cloud coverage: 2019 06 22 with  95.609655 pct | nodata pixel pct: 0.121118\n",
      "passed cloud coverage: 2019 06 25 with 31.599539999999998 pct | nodata pixel pct: 5.988713\n",
      "passed cloud coverage: 2019 07 02 with 0.438801 pct | nodata pixel pct: 0.100594\n",
      "FAILED cloud coverage: 2019 07 05 with  99.99907 pct | nodata pixel pct: 6.151343\n",
      "passed cloud coverage: 2019 07 12 with 3.086262 pct | nodata pixel pct: 0.086267\n",
      "passed cloud coverage: 2019 07 15 with 21.64587 pct | nodata pixel pct: 6.209598\n",
      "passed cloud coverage: 2019 07 22 with 1.7739530000000001 pct | nodata pixel pct: 0.070939\n",
      "FAILED cloud coverage: 2019 07 25 with  86.924583 pct | nodata pixel pct: 6.315012\n",
      "passed cloud coverage: 2019 08 01 with 21.355782 pct | nodata pixel pct: 0.071111\n",
      "passed cloud coverage: 2019 08 04 with 31.616823000000004 pct | nodata pixel pct: 6.318715\n",
      "passed cloud coverage: 2019 08 31 with 11.996053 pct | nodata pixel pct: 0.08213\n",
      "FAILED cloud coverage: 2019 09 03 with  96.468214 pct | nodata pixel pct: 6.272218\n",
      "FAILED cloud coverage: 2019 09 10 with  98.410389 pct | nodata pixel pct: 0.093722\n",
      "FAILED cloud coverage: 2019 09 13 with  77.084971 pct | nodata pixel pct: 5.567715\n",
      "FAILED cloud coverage: 2019 09 20 with  67.213582 pct | nodata pixel pct: 0.144754\n",
      "FAILED cloud coverage: 2019 09 30 with  78.85864 pct | nodata pixel pct: 0.117634\n",
      "FAILED cloud coverage: 2019 10 03 with  99.54555499999998 pct | nodata pixel pct: 6.040292\n",
      "FAILED cloud coverage: 2019 10 10 with  99.351898 pct | nodata pixel pct: 0.105726\n",
      "FAILED cloud coverage: 2019 10 13 with  90.873236 pct | nodata pixel pct: 6.083759\n",
      "FAILED cloud coverage: 2019 10 20 with  99.999943 pct | nodata pixel pct: 0.082803\n",
      "FAILED cloud coverage: 2019 10 23 with  96.807935 pct | nodata pixel pct: 6.183188\n",
      "FAILED cloud coverage: 2019 10 30 with  96.622765 pct | nodata pixel pct: 0.079064\n",
      "FAILED cloud coverage: 2019 05 01 with  87.732914 pct | nodata pixel pct: 6.185328\n",
      "FAILED cloud coverage: 2019 05 30 with  100.0 pct | nodata pixel pct: 0.056058\n",
      "FAILED cloud coverage: 2019 05 11 with  58.859754 pct | nodata pixel pct: 6.305656\n",
      "FAILED cloud coverage: 2019 05 18 with  90.59443099999999 pct | nodata pixel pct: 0.063185\n",
      "FAILED cloud coverage: 2019 05 21 with  99.99877 pct | nodata pixel pct: 6.271416\n",
      "FAILED cloud coverage: 2019 05 28 with  52.279438 pct | nodata pixel pct: 0.073311\n",
      "passed cloud coverage: 2019 05 31 with 16.165152 pct | nodata pixel pct: 6.122601\n",
      "passed cloud coverage: 2019 06 07 with 34.25434 pct | nodata pixel pct: 0.084359\n",
      "passed cloud coverage: 2019 06 10 with 38.334692 pct | nodata pixel pct: 5.996543\n",
      "FAILED cloud coverage: 2019 06 17 with  99.23411600000001 pct | nodata pixel pct: 0.099343\n",
      "FAILED cloud coverage: 2019 06 20 with  99.779534 pct | nodata pixel pct: 6.049051\n",
      "FAILED cloud coverage: 2019 06 27 with  99.999997 pct | nodata pixel pct: 0.079011\n",
      "passed cloud coverage: 2019 06 30 with 34.567865 pct | nodata pixel pct: 6.143049\n",
      "passed cloud coverage: 2019 07 07 with 0.247529 pct | nodata pixel pct: 0.065132\n",
      "FAILED cloud coverage: 2019 07 10 with  68.309968 pct | nodata pixel pct: 6.260981\n",
      "passed cloud coverage: 2019 07 17 with 36.433911 pct | nodata pixel pct: 0.067764\n",
      "passed cloud coverage: 2019 07 20 with 14.27202 pct | nodata pixel pct: 6.286728\n",
      "passed cloud coverage: 2019 07 27 with 7.542372000000001 pct | nodata pixel pct: 0.066267\n",
      "passed cloud coverage: 2019 07 30 with 0.035444 pct | nodata pixel pct: 6.234143\n",
      "passed cloud coverage: 2019 08 06 with 0.038382 pct | nodata pixel pct: 0.06491\n",
      "FAILED cloud coverage: 2019 08 26 with  95.0637 pct | nodata pixel pct: 0.088109\n",
      "passed cloud coverage: 2019 08 29 with 30.263980000000004 pct | nodata pixel pct: 5.974529\n",
      "FAILED cloud coverage: 2019 09 05 with  88.658589 pct | nodata pixel pct: 0.095315\n",
      "passed cloud coverage: 2019 09 08 with 36.874535 pct | nodata pixel pct: 5.933003\n",
      "passed cloud coverage: 2019 09 15 with 8.414777 pct | nodata pixel pct: 0.117153\n",
      "passed cloud coverage: 2019 09 18 with 5.33546 pct | nodata pixel pct: 5.332457\n",
      "passed cloud coverage: 2019 09 25 with 5.946447 pct | nodata pixel pct: 0.1949\n",
      "FAILED cloud coverage: 2019 09 28 with  52.442567000000004 pct | nodata pixel pct: 5.425321\n",
      "FAILED cloud coverage: 2019 10 05 with  85.82431 pct | nodata pixel pct: 93.368345\n",
      "FAILED cloud coverage: 2019 10 05 with  90.76353500000002 pct | nodata pixel pct: 0.1499\n",
      "passed cloud coverage: 2019 10 08 with 0.08202 pct | nodata pixel pct: 5.618893\n",
      "FAILED cloud coverage: 2019 10 15 with  87.448669 pct | nodata pixel pct: 0.138497\n",
      "passed cloud coverage: 2019 10 18 with 3.4669 pct | nodata pixel pct: 5.782141\n",
      "FAILED cloud coverage: 2019 10 25 with  71.940527 pct | nodata pixel pct: 0.129004\n",
      "passed cloud coverage: 2019 10 28 with 11.589603 pct | nodata pixel pct: 5.773577\n",
      "30 out of 67 time steps passed cloud coverage threshold\n",
      "<xarray.Dataset>\n",
      "Dimensions:  (time: 30, x: 500, y: 500)\n",
      "Coordinates:\n",
      "  * y        (y) float64 5.225e+06 5.225e+06 5.225e+06 ... 5.22e+06 5.22e+06\n",
      "  * x        (x) float64 6.75e+05 6.75e+05 6.75e+05 ... 6.8e+05 6.8e+05 6.8e+05\n",
      "  * time     (time) datetime64[ns] 2019-05-13 2019-05-31 ... 2019-10-28\n",
      "Data variables:\n",
      "    red      (time, y, x) float64 dask.array<chunksize=(30, 500, 500), meta=np.ndarray>\n",
      "    green    (time, y, x) float64 dask.array<chunksize=(30, 500, 500), meta=np.ndarray>\n",
      "    blue     (time, y, x) float64 dask.array<chunksize=(30, 500, 500), meta=np.ndarray>\n",
      "    nir      (time, y, x) float64 dask.array<chunksize=(30, 500, 500), meta=np.ndarray>\n",
      "Attributes:\n",
      "    transform:     (10.0, 0.0, 600000.0, 0.0, -10.0, 5300040.0)\n",
      "    crs:           +init=epsg:32614\n",
      "    res:           (10.0, 10.0)\n",
      "    is_tiled:      1\n",
      "    nodatavals:    (nan, nan, nan, nan)\n",
      "    scales:        (1.0, 1.0, 1.0, 1.0)\n",
      "    offsets:       (0.0, 0.0, 0.0, 0.0)\n",
      "    descriptions:  ('B4, central wavelength 665 nm', 'B3, central wavelength ...\n",
      "    tile_id:       T14TPT\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "### Loaing data from a directory of zipped Sentinel-2 files\n",
    "\n",
    "ds_time_stack = prep_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 237.80 MiB, increment: 0.00 MiB\n",
      "Wall time: 5.63 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%memit\n",
    "\n",
    "## Example code for running Thresholding approach\n",
    "## These functions are not integrated into the global config, so variables have to be defined before the function call\n",
    "\n",
    "### Set file paths for read and write\n",
    "file_out_str = '_code_demo_threshold'\n",
    "out_dir = config['shp_out_dir']\n",
    "tile_id = config['prep_tile_id']\n",
    "raster_dir = config['prep_file_dir']\n",
    "x_start = config['prep_x_start']\n",
    "y_start = config['prep_y_start']\n",
    "step = config['prep_step']\n",
    "base_chunk = 'auto'\n",
    "\n",
    "## Data read parameters\n",
    "cloud_thresh = 20\n",
    "\n",
    "## Crop maask parameters\n",
    "ndwi_thresh = 0.5 \n",
    "ndvi_max_thresh = 0.5\n",
    "ndvi_range_thresh = 0.5 \n",
    "edges_thresh = 0.3\n",
    "\n",
    "# set rgb date string from the dict\n",
    "rgb_date_str = rgb_date_dict[tile_id]\n",
    "rgb_gaussian_sigma = config['seg_rgb_gaussian_sigma']\n",
    "fz_scale = config['seg_fz_scale']\n",
    "fz_sigma = config['seg_fz_sigma']\n",
    "fz_min_size = config['seg_fz_min_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 15 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# %memit\n",
    "ndwi_mask = ndwi_mask_func(ds_time_stack, ndwi_thresh = ndwi_thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 32.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ndwi_mask_compute = ndwi_mask_func(ds_time_stack, ndwi_thresh = ndwi_thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 930.60 MiB, increment: -0.17 MiB\n"
     ]
    }
   ],
   "source": [
    "%memit"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
